# üöÄ Pipeline de An√°lise de Sentimentos e T√≥picos - E-commerce

Este reposit√≥rio cont√©m o pipeline completo de Engenharia de Dados e NLP para o **Par Tem√°tico 2: An√°lise de Sentimentos no E-commerce Brasileiro**. O objetivo do projeto √© coletar, processar, analisar e persistir reviews de produtos de um grande varejista brasileiro (Casas Bahia).

O pipeline final processa os dados brutos e gera um banco de dados (`reviews.duckdb`) pronto para ser consumido por uma ferramenta de BI (como o Tableau) para an√°lise gerencial.

## üìà Pipeline de Processamento

O projeto √© dividido em um pipeline de ponta a ponta:

1.  **Coleta de Dados (Scraping):** Os scripts em `src/scraping/` s√£o respons√°veis por coletar dados do site-alvo e salvar os coment√°rios brutos.
2.  **Limpeza e Processamento:** O texto dos coment√°rios √© normalizado (acentos, caixa baixa, caracteres especiais) para preparar a an√°lise.
3.  **An√°lise de Sentimento:** Um modelo h√≠brido (baseado em regras de nota e no l√©xico `LeIA`) classifica cada coment√°rio como `Positivo`, `Negativo` ou `Neutro`.
4.  **Extra√ß√£o de T√≥picos:** A biblioteca `spaCy` √© usada para analisar os coment√°rios e extrair os substantivos-chave (ex: "bateria", "tela", "entrega"), indicando *sobre o que* o cliente falou.
5.  **Persist√™ncia (Data Warehouse):** O resultado final (coment√°rios, sentimentos e t√≥picos) √© salvo em um banco de dados `DuckDB`, pronto para a an√°lise de BI.

## üóÇÔ∏è Estrutura do Reposit√≥rio

O projeto √© organizado de forma modular para garantir a separa√ß√£o entre c√≥digo-fonte, dados e notebooks de an√°lise, conforme a estrutura final do projeto.

```
bi-web-scrapping/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md               <-- Este arquivo
‚îú‚îÄ‚îÄ requirements.txt        <-- Todas as depend√™ncias do projeto
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                <-- Dados brutos coletados (ex: comentarios_produtos.csv)
‚îÇ   ‚îî‚îÄ‚îÄ output/             <-- Sa√≠da do pipeline (ex: reviews.duckdb)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 1-analise_sentimento_mvp.ipynb  <-- Entrega da Semana 2
‚îÇ   ‚îî‚îÄ‚îÄ 2-extracao_topicos_e_db.ipynb   <-- Entrega da Semana 3 (Pipeline Completo)
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ processing/         <-- M√≥dulo Python com toda a l√≥gica de NLP e BD
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_processor.py
‚îÇ   ‚îî‚îÄ‚îÄ scraping/           <-- M√≥dulo Python com os scripts de coleta de dados
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ agents.py
‚îÇ       ‚îú‚îÄ‚îÄ get_products.py
‚îÇ       ‚îî‚îÄ‚îÄ get_reviews.py
‚îÇ
‚îî‚îÄ‚îÄ venv/                   <-- Ambiente virtual (ignorado pelo .gitignore)
```

## ‚öôÔ∏è Como Configurar e Executar o Projeto

Siga estes passos para configurar o ambiente e executar o pipeline completo.

### 1. Pr√©-requisitos

* **Python:** Este projeto foi desenvolvido e testado com **Python 3.11**. Vers√µes mais novas (como 3.13+) causam erros de incompatibilidade com as bibliotecas de dados.
* **Git:** Para clonar o reposit√≥rio.
* **Habilitar Caminhos Longos no Windows:** Este projeto exige a instala√ß√£o de pacotes com nomes de arquivo longos. √â **obrigat√≥rio** habilitar o suporte a "Win32 Long Paths" no Windows.

### 2. Instala√ß√£o

**Passo 1: Clonar o Reposit√≥rio**
```bash
git clone [https://github.com/seu-usuario/bi-web-scrapping.git](https://github.com/seu-usuario/bi-web-scrapping.git)
cd bi-web-scrapping
```

**Passo 2: Criar e Ativar o Ambiente Virtual**
(√â crucial usar o Python 3.11 para este comando)
```bash
# Cria o ambiente virtual
py -3.11 -m venv venv

# Ativa o ambiente (no terminal Bash do VS Code)
source venv/Scripts/activate

# (Se estiver usando o CMD Padr√£o do Windows, use: .\venv\Scripts\activate)
```

**Passo 3: Instalar todas as Depend√™ncias**
(Com o ambiente `(venv)` ativo)
```bash
# Instala todas as bibliotecas do projeto
pip install -r requirements.txt

# Instala a ferramenta Jupyter Notebook
pip install notebook
```

**Passo 4: Baixar o Modelo de NLP (spaCy)**
```bash
python -m spacy download pt_core_news_sm
```

### 3. Executando o Pipeline

O pipeline de processamento (Semanas 2 e 3) √© executado atrav√©s do notebook principal.

**Passo 1: Iniciar o Jupyter Notebook**
(Com o ambiente `(venv)` ativo)
```bash
# Comando mais robusto para iniciar o notebook
python -m notebook
```

**Passo 2: Executar o Notebook da Semana 3**
1.  No navegador que abrir, clique na pasta `notebooks/`.
2.  Abra o arquivo `2-extracao_topicos_e_db.ipynb`.
3.  **REINICIE O KERNEL:** V√° ao menu **"Kernel" > "Restart Kernel..."** (para garantir que todas as bibliotecas instaladas sejam carregadas).
4.  Execute todas as c√©lulas do notebook, de cima para baixo.

### 4. Sa√≠da do Projeto

Ap√≥s a execu√ß√£o bem-sucedida, o arquivo final **`reviews.duckdb`** estar√° dispon√≠vel na pasta `data/output/`.

Este arquivo cont√©m a tabela `reviews_classificadas` com todas as colunas, incluindo `sentimento` e `topicos`, pronta para ser conectada ao Tableau.