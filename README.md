# üöÄ Pipeline de An√°lise de Sentimentos e T√≥picos - E-commerce

Este reposit√≥rio cont√©m o pipeline completo de Engenharia de Dados e NLP para o **Par Tem√°tico 2: An√°lise de Sentimentos no E-commerce Brasileiro**. O objetivo do projeto √© coletar, processar, analisar e persistir reviews de produtos de um grande varejista brasileiro (Casas Bahia).

O pipeline final processa os dados brutos e gera um banco de dados (`reviews.duckdb`) pronto para ser consumido por uma ferramenta de BI (como o Tableau) para an√°lise gerencial.

## üìà Pipeline de Processamento

O projeto √© dividido em um pipeline de ponta a ponta:

1.  **Coleta de Dados (Scraping):** Os scripts em `src/scraping/` s√£o respons√°veis por coletar dados do site-alvo e salvar os coment√°rios brutos.
2.  **Limpeza e Processamento:** O texto dos coment√°rios √© normalizado (acentos, caixa baixa, caracteres especiais) para preparar a an√°lise.
3.  **An√°lise de Sentimento:** Um modelo h√≠brido (baseado em regras de nota e no l√©xico `LeIA`) classifica cada coment√°rio como `Positivo`, `Negativo` ou `Neutro`.
4.  **Extra√ß√£o de T√≥picos:** A biblioteca `spaCy` √© usada para analisar os coment√°rios e extrair os substantivos-chave (ex: "bateria", "tela", "entrega"), indicando *sobre o que* o cliente falou.
5.  **Extra√ß√£o de Palavras:** O script `extract_words_from_comments.py` processa os coment√°rios classificados e extrai as palavras individuais, gerando dois arquivos CSV separados por sentimento (positivo e negativo) com as colunas `palavra` e `id_comentario`.
6.  **Persist√™ncia (Data Warehouse):** O resultado final (coment√°rios, sentimentos e t√≥picos) √© salvo em um banco de dados `DuckDB`, pronto para a an√°lise de BI.

## üóÇÔ∏è Estrutura do Reposit√≥rio

O projeto √© organizado de forma modular para garantir a separa√ß√£o entre c√≥digo-fonte, dados e notebooks de an√°lise, conforme a estrutura final do projeto.

```
bi-web-scrapping/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md               <-- Este arquivo
‚îú‚îÄ‚îÄ requirements.txt        <-- Todas as depend√™ncias do projeto
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                <-- Dados brutos coletados (ex: comentarios_produtos.csv)
‚îÇ   ‚îî‚îÄ‚îÄ output/             <-- Sa√≠da do pipeline (ex: reviews.duckdb)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 1-analise_sentimento_mvp.ipynb  <-- Entrega da Semana 2
‚îÇ   ‚îî‚îÄ‚îÄ 2-extracao_topicos_e_db.ipynb   <-- Entrega da Semana 3 (Pipeline Completo)
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ processing/         <-- M√≥dulo Python com toda a l√≥gica de NLP e BD
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_processor.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ extract_words_from_comments.py  <-- Script para extrair palavras dos coment√°rios
‚îÇ   ‚îî‚îÄ‚îÄ scraping/           <-- M√≥dulo Python com os scripts de coleta de dados
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ agents.py
‚îÇ       ‚îú‚îÄ‚îÄ get_products.py
‚îÇ       ‚îî‚îÄ‚îÄ get_reviews.py
‚îÇ
‚îî‚îÄ‚îÄ venv/                   <-- Ambiente virtual (ignorado pelo .gitignore)
```

## ‚öôÔ∏è Como Configurar e Executar o Projeto

Siga estes passos para configurar o ambiente e executar o pipeline completo.

### 1. Pr√©-requisitos

* **Python:** Este projeto foi desenvolvido e testado com **Python 3.11**. Vers√µes mais novas (como 3.13+) causam erros de incompatibilidade com as bibliotecas de dados.
* **Git:** Para clonar o reposit√≥rio.
* **Habilitar Caminhos Longos no Windows:** Este projeto exige a instala√ß√£o de pacotes com nomes de arquivo longos. √â **obrigat√≥rio** habilitar o suporte a "Win32 Long Paths" no Windows.

### 2. Instala√ß√£o

**Passo 1: Clonar o Reposit√≥rio**
```bash
git clone [https://github.com/seu-usuario/bi-web-scrapping.git](https://github.com/seu-usuario/bi-web-scrapping.git)
cd bi-web-scrapping
```

**Passo 2: Criar e Ativar o Ambiente Virtual**
(√â crucial usar o Python 3.11 para este comando)
```bash
# Cria o ambiente virtual
py -3.11 -m venv venv

# Ativa o ambiente (no terminal Bash do VS Code)
source venv/Scripts/activate

# (Se estiver usando o CMD Padr√£o do Windows, use: .\venv\Scripts\activate)
```

**Passo 3: Instalar todas as Depend√™ncias**
(Com o ambiente `(venv)` ativo)
```bash
# Instala todas as bibliotecas do projeto
pip install -r requirements.txt

# Instala a ferramenta Jupyter Notebook
pip install notebook
```

**Passo 4: Baixar o Modelo de NLP (spaCy)**
```bash
python -m spacy download pt_core_news_sm
```

### 3. Executando o Pipeline

O pipeline de processamento (Semanas 2 e 3) √© executado atrav√©s do notebook principal.

**Passo 1: Iniciar o Jupyter Notebook**
(Com o ambiente `(venv)` ativo)
```bash
# Comando mais robusto para iniciar o notebook
python -m notebook
```

**Passo 2: Executar o Notebook da Semana 3**
1.  No navegador que abrir, clique na pasta `notebooks/`.
2.  Abra o arquivo `2-extracao_topicos_e_db.ipynb`.
3.  **REINICIE O KERNEL:** V√° ao menu **"Kernel" > "Restart Kernel..."** (para garantir que todas as bibliotecas instaladas sejam carregadas).
4.  Execute todas as c√©lulas do notebook, de cima para baixo.

### 4. Extra√ß√£o de Palavras dos Coment√°rios (An√°lise Complementar)

Ap√≥s executar o pipeline completo, voc√™ pode extrair as palavras individuais dos coment√°rios classificados, gerando dois arquivos CSV separados por sentimento.

**Como Executar:**
(Com o ambiente `(venv)` ativo, na pasta raiz do projeto)

```bash
python src/processing/extract_words_from_comments.py
```

**O que este script faz:**

1. L√™ o arquivo `comentarios_classificados.csv` gerado pelo pipeline.
2. Separa os coment√°rios por sentimento (Positivos e Negativos).
3. Extrai todas as palavras de cada coment√°rio:
   - Remove pontua√ß√£o e caracteres especiais
   - Converte para min√∫sculas
   - Mant√©m suporte a acentos (√°√©√≠√≥√∫√£√µ√ß√†)
4. Gera dois arquivos CSV na pasta `data/output/`:
   - **`palavras_positivas.csv`** - Palavras dos coment√°rios positivos
   - **`palavras_negativas.csv`** - Palavras dos coment√°rios negativos

Ambos os arquivos possuem as colunas:
- `palavra`: A palavra extra√≠da do coment√°rio
- `id_comentario`: O identificador √∫nico do coment√°rio original

Esta an√°lise √© √∫til para identificar quais palavras mais frequentemente aparecem em coment√°rios positivos versus negativos, ajudando na an√°lise de sentimentos e no entendimento das prefer√™ncias dos clientes.

### 5. Sa√≠da do Projeto

Ap√≥s a execu√ß√£o bem-sucedida, o arquivo final **`reviews.duckdb`** estar√° dispon√≠vel na pasta `data/output/`.

Este arquivo cont√©m a tabela `reviews_classificadas` com todas as colunas, incluindo `sentimento` e `topicos`, pronta para ser conectada ao Tableau.

Adicionalmente, ap√≥s executar o script de extra√ß√£o de palavras, voc√™ ter√° dispon√≠veis:
- **`palavras_positivas.csv`** - Palavras extra√≠das dos coment√°rios positivos
- **`palavras_negativas.csv`** - Palavras extra√≠das dos coment√°rios negativos